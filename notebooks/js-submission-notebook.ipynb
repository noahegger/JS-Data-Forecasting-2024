{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-15T18:19:32.510681Z",
     "iopub.status.busy": "2024-12-15T18:19:32.510255Z",
     "iopub.status.idle": "2024-12-15T18:19:33.687303Z",
     "shell.execute_reply": "2024-12-15T18:19:33.686245Z"
    },
    "papermill": {
     "duration": 1.18424,
     "end_time": "2024-12-15T18:19:33.689733",
     "exception": false,
     "start_time": "2024-12-15T18:19:32.505493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import defaultdict, deque\n",
    "from pathlib import Path\n",
    "from typing import IO, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a global variable for the project path\n",
    "DATA_DIR = Path(\"/kaggle/input/jane-street-real-time-market-data-forecasting\")\n",
    "LOCAL_DATA_DIR = Path(\"/Users/noahegger/git/JS-Data-Forecasting-2024\")\n",
    "PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "sys.path.append(str(PROJECT_PATH))\n",
    "\n",
    "META_COLS = [\"date_id\", \"time_id\", \"symbol_id\", \"weight\"]\n",
    "FEATURE_COLS = [f\"feature_{x:02}\" for x in range(79)]\n",
    "RESPONDER_COLS = [f\"responder_{i}\" for i in range(9)]\n",
    "\n",
    "from scripts.calculators import (\n",
    "    ExpWeightedMeanCalculator,\n",
    "    MovingAverageCalculator,\n",
    "    OnlineMovingAverageCalculator,\n",
    "    RevDecayCalculator,\n",
    ")\n",
    "from scripts.models import BaseModel, EnsembleTimeSeriesV1\n",
    "from scripts.preprocessing import Preprocessor\n",
    "from scripts.record import Cache, SymbolRecord\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.001737,
     "end_time": "2024-12-15T18:19:33.693773",
     "exception": false,
     "start_time": "2024-12-15T18:19:33.692036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The evaluation API requires that you set up a server which will respond to inference requests. We have already defined the server; you just need write the predict function. When we evaluate your submission on the hidden test set the client defined in `jane_street_gateway` will run in a different container with direct access to the hidden test set and hand off the data timestep by timestep.\n",
    "\n",
    "\n",
    "\n",
    "Your code will always have access to the published copies of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class InferenceServer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        preprocessor,\n",
    "        dir: Optional[Path] = None,\n",
    "        test_parquet: Optional[str] = None,\n",
    "        lag_parquet: Optional[str] = None,\n",
    "        cache_lb_days: int = 15,\n",
    "        feature_cols: List[str] = FEATURE_COLS,\n",
    "        responder_cols: List[str] = RESPONDER_COLS,\n",
    "        test: bool = False,\n",
    "        partition_ids: Optional[List[int]] = None,\n",
    "        synthetic_days: int = 50,\n",
    "        num_score_dates: int = 5,\n",
    "        date_offset: int = 1690,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.dir = dir\n",
    "        self.test_parquet = str(test_parquet) if test_parquet else \"\"\n",
    "        self.lag_parquet = str(lag_parquet) if lag_parquet else \"\"\n",
    "        self.cache_lb_days = cache_lb_days\n",
    "        self.feature_cols = feature_cols\n",
    "        self.responder_cols = responder_cols\n",
    "        self.cache_history = Cache(maxlen=self.cache_lb_days)\n",
    "        self.lag_cache = Cache(maxlen=self.cache_lb_days)\n",
    "        self.test_ = None\n",
    "        self.time_step_count = 0\n",
    "        self.test = test\n",
    "        self.partition_ids = partition_ids\n",
    "        self.synthetic_days = synthetic_days\n",
    "        self.num_score_dates = num_score_dates\n",
    "        self.date_offset = date_offset\n",
    "\n",
    "        self.ensure_parquet_files()\n",
    "        self.total_time_steps = self.calculate_total_time_steps()\n",
    "        self.initialize_pbar()\n",
    "\n",
    "        if not self.test:\n",
    "            last_train_set = pl.read_parquet(\n",
    "                f\"{self.dir}/train.parquet/partition_id=9/part-0.parquet\"\n",
    "            )\n",
    "            self.initialize_cache(\n",
    "                last_train_set.filter(\n",
    "                    pl.col(\"date_id\") >= (self.date_offset - self.cache_lb_days)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def ensure_parquet_files(self):\n",
    "        if self.test and (\n",
    "            not self.test_parquet\n",
    "            or not Path(self.test_parquet).exists()\n",
    "            or not self.lag_parquet\n",
    "            or not Path(self.lag_parquet).exists()\n",
    "        ):\n",
    "            self.test_parquet, self.lag_parquet = self.generate_synthetic_data()\n",
    "\n",
    "    def generate_synthetic_data(self):\n",
    "        train_parquets = [\n",
    "            f\"{self.dir}/train.parquet/partition_id={i}/part-0.parquet\"\n",
    "            for i in (self.partition_ids or range(10))\n",
    "        ]\n",
    "        train_data = pl.concat([pl.read_parquet(parquet) for parquet in train_parquets])\n",
    "\n",
    "        if self.synthetic_days:\n",
    "            date_ids = sorted(train_data[\"date_id\"].unique())[: self.synthetic_days]\n",
    "            test_data, lag_data = (\n",
    "                train_data.filter(pl.col(\"date_id\").is_in(date_ids)),\n",
    "                train_data.filter(pl.col(\"date_id\").is_in(date_ids)),\n",
    "            )\n",
    "        else:\n",
    "            test_data, lag_data = train_data, train_data\n",
    "\n",
    "        # Adjust date_id for test_data and lag_data\n",
    "        test_data = test_data.with_columns(\n",
    "            (pl.col(\"date_id\") - self.cache_lb_days).alias(\"date_id\")\n",
    "        ).with_row_index(name=\"row_id\", offset=0)\n",
    "\n",
    "        lag_data = lag_data.with_columns(\n",
    "            (pl.col(\"date_id\") - self.cache_lb_days).alias(\"date_id\")\n",
    "        )\n",
    "\n",
    "        # Add \"is_scored\" column to test_data\n",
    "        test_data = test_data.with_columns(\n",
    "            pl.when(pl.col(\"date_id\") < 0)\n",
    "            .then(False)\n",
    "            .otherwise(True)\n",
    "            .alias(\"is_scored\")\n",
    "        )\n",
    "\n",
    "        # Initialize the cache with test data where date_id < 0\n",
    "        self.initialize_cache(test_data.filter(pl.col(\"date_id\") < 0))\n",
    "\n",
    "        # Select relevant columns for test_data and lag_data\n",
    "        test_data = test_data.select(\n",
    "            [\"row_id\"] + META_COLS + [\"is_scored\"] + FEATURE_COLS\n",
    "        )\n",
    "        lag_data = lag_data.select(META_COLS + RESPONDER_COLS)\n",
    "\n",
    "        # Define paths for synthetic data\n",
    "        test_parquet_path = Path(self.test_parquet)\n",
    "        lag_parquet_path = Path(self.lag_parquet)\n",
    "\n",
    "        # Create directories if they don't exist\n",
    "        test_parquet_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        lag_parquet_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Partition and save test_data\n",
    "        test_data_partition = test_data.partition_by(\n",
    "            \"date_id\", maintain_order=True, as_dict=True\n",
    "        )\n",
    "        row_id_offset = (\n",
    "            test_data.filter(pl.col(\"date_id\") < 0).select(\"row_id\").max().item()\n",
    "        )\n",
    "\n",
    "        for key, _df in test_data_partition.items():\n",
    "            date_id = key[0]\n",
    "            if date_id >= 0:  # type: ignore\n",
    "                partition_dir = test_parquet_path / f\"date_id={date_id}\"\n",
    "                partition_dir.mkdir(parents=True, exist_ok=True)\n",
    "                _df = _df.with_columns(pl.col(\"row_id\") - row_id_offset)\n",
    "                _df.write_parquet(partition_dir / \"part-0.parquet\")\n",
    "\n",
    "        # Partition and save lag_data\n",
    "        lag_data = lag_data.rename(\n",
    "            {f\"responder_{x}\": f\"responder_{x}_lag_1\" for x in range(9)}\n",
    "        )\n",
    "        lag_data_partition = lag_data.partition_by(\n",
    "            \"date_id\", maintain_order=True, as_dict=True\n",
    "        )\n",
    "\n",
    "        for key, _df in lag_data_partition.items():\n",
    "            date_id = key[0] + 1  # type: ignore\n",
    "            if date_id >= 0:  # type: ignore\n",
    "                partition_dir = lag_parquet_path / f\"date_id={date_id}\"\n",
    "                partition_dir.mkdir(parents=True, exist_ok=True)\n",
    "                _df = _df.with_columns((pl.col(\"date_id\") + 1).alias(\"date_id\"))\n",
    "                _df.write_parquet(partition_dir / \"part-0.parquet\")\n",
    "\n",
    "        return str(test_parquet_path), str(lag_parquet_path)\n",
    "\n",
    "    def calculate_total_time_steps(self):\n",
    "        if self.test and self.test_parquet:\n",
    "            return (\n",
    "                pl.scan_parquet(self.test_parquet)\n",
    "                .select((pl.col(\"date_id\") * 10000 + pl.col(\"time_id\")).n_unique())\n",
    "                .collect()\n",
    "                .item()\n",
    "            )\n",
    "        return 1  # For live submission\n",
    "\n",
    "    def initialize_pbar(self):\n",
    "        self.pbar = tqdm(total=self.total_time_steps)\n",
    "\n",
    "    def generate_data_batches(self, test_data: pl.DataFrame, lag_data: pl.DataFrame):\n",
    "        date_ids = sorted(test_data[\"date_id\"].unique())\n",
    "        assert date_ids[0] == 0\n",
    "\n",
    "        for date_id in date_ids:\n",
    "            test_batches = test_data.filter(pl.col(\"date_id\") == date_id).group_by(\n",
    "                \"time_id\", maintain_order=True\n",
    "            )\n",
    "            lags = lag_data.filter(pl.col(\"date_id\") == date_id)\n",
    "\n",
    "            for (time_id,), test in test_batches:\n",
    "                test_batch = (test, lags if time_id == 0 else None)\n",
    "                validation_data = test.select(\"row_id\")  # row_id in gateway?\n",
    "                yield test_batch, validation_data\n",
    "\n",
    "    def run_inference_server(self):\n",
    "        self.pbar.refresh()\n",
    "\n",
    "        if self.test:\n",
    "            if self.lag_parquet and self.test_parquet:\n",
    "                lag_data = pl.scan_parquet(f\"{self.lag_parquet}/**/*.parquet\").collect()\n",
    "                test_data = pl.scan_parquet(\n",
    "                    f\"{self.test_parquet}/**/*.parquet\"\n",
    "                ).collect()\n",
    "            else:\n",
    "                raise ValueError(\"lag_parquet or test_parquet is None\")\n",
    "            for test_batch, validation_data in self.generate_data_batches(\n",
    "                test_data, lag_data\n",
    "            ):\n",
    "                test, lags = test_batch\n",
    "                if lags is None:\n",
    "                    lags = pl.DataFrame()\n",
    "                self.predict(test, lags)\n",
    "        else:\n",
    "            import kaggle_evaluation.jane_street_inference_server as js_server\n",
    "\n",
    "            inference_server = js_server.JSInferenceServer((self.predict,))\n",
    "\n",
    "            if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "                inference_server.serve()\n",
    "            else:\n",
    "                inference_server.run_local_gateway(\n",
    "                    (self.test_parquet, self.lag_parquet)\n",
    "                )\n",
    "\n",
    "        self.pbar.close()\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        test: pl.DataFrame,\n",
    "        lags: Optional[pl.DataFrame] = None,\n",
    "    ) -> pl.DataFrame | pd.DataFrame:\n",
    "\n",
    "        if lags is not None:\n",
    "            lag_responder_cols = [f\"{col}_lag_1\" for col in self.responder_cols]\n",
    "            for (symbol_id,), batch in lags.group_by(\"symbol_id\", maintain_order=True):\n",
    "\n",
    "                batch_data = batch.select(META_COLS + lag_responder_cols)\n",
    "                date_id = batch[\"date_id\"][0]\n",
    "                self.lag_cache.update(symbol_id, date_id, batch_data, is_lag_cache=True)  # type: ignore\n",
    "\n",
    "        self.test_ = test\n",
    "\n",
    "        if test[\"is_scored\"].any():\n",
    "            try:\n",
    "                symbol_ids = []\n",
    "                row_ids = []\n",
    "                for (symbol_id,), batch in test.group_by(\n",
    "                    \"symbol_id\", maintain_order=True\n",
    "                ):\n",
    "                    batch_data = batch.select(META_COLS + self.feature_cols)\n",
    "                    date_id = batch[\"date_id\"][0]\n",
    "                    self.cache_history.update(symbol_id, date_id, batch_data)  # type: ignore\n",
    "\n",
    "                    symbol_ids.append(symbol_id)\n",
    "                    row_ids.append(batch[\"row_id\"][-1])\n",
    "\n",
    "                # Extract the most recent date_id\n",
    "                tdate = test[\"date_id\"][-1]\n",
    "                ttime = batch[\"time_id\"][-1]\n",
    "                # Pass the cache history to the prediction model\n",
    "                estimates = self.model.get_estimates(\n",
    "                    symbol_ids=symbol_ids,\n",
    "                    cache_history=self.cache_history.cache,\n",
    "                    lag_cache=self.lag_cache.cache,\n",
    "                    tdate=tdate,\n",
    "                    ttime=ttime,\n",
    "                )\n",
    "\n",
    "                predictions = pl.DataFrame(\n",
    "                    {\n",
    "                        \"row_id\": row_ids,\n",
    "                        \"responder_6\": estimates,\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "        else:\n",
    "            predictions = pl.DataFrame(\n",
    "                {\"row_id\": test[\"row_id\"], \"responder_6\": [0] * len(test)}\n",
    "            )\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def initialize_cache(self, data: pl.DataFrame):\n",
    "        self.cache_history.initialize(data, META_COLS + self.feature_cols)\n",
    "        self.lag_cache.initialize(data, META_COLS + self.responder_cols, lagged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "LOCAL_TEST = False\n",
    "KAGGLE_TEST = False\n",
    "\n",
    "model = EnsembleTimeSeriesV1(\n",
    "    online_feature=OnlineMovingAverageCalculator(window=10),\n",
    "    long_term_feature=ExpWeightedMeanCalculator(halflife=0.35, lookback=15),\n",
    "    rev_decay_calculator=RevDecayCalculator(lookback=15),\n",
    "    st_window=15,\n",
    "    lt_window=15,\n",
    ")\n",
    "preprocessor = Preprocessor(\n",
    "    symbol_id=None,\n",
    "    responder=6,\n",
    "    partition_ids=None,\n",
    "    feature_set=None,\n",
    "    sample_frequency=15,\n",
    "    exclude_set=[\n",
    "        \"feature_00\",\n",
    "        \"feature_01\",\n",
    "        \"feature_02\",\n",
    "        \"feature_03\",\n",
    "        \"feature_04\",\n",
    "        \"feature_21\",\n",
    "        \"feature_26\",\n",
    "        \"feature_27\",\n",
    "        \"feature_31\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "if LOCAL_TEST:\n",
    "    inference_server = InferenceServer(\n",
    "        model=model,\n",
    "        preprocessor=preprocessor,\n",
    "        dir=LOCAL_DATA_DIR,\n",
    "        test_parquet=f\"{LOCAL_DATA_DIR}/synthetic_test.parquet\",\n",
    "        lag_parquet=f\"{LOCAL_DATA_DIR}/synthetic_lag.parquet\",\n",
    "        cache_lb_days=15,\n",
    "        feature_cols=FEATURE_COLS,\n",
    "        responder_cols=RESPONDER_COLS,\n",
    "        test=True,  # Set to True for local testing\n",
    "        partition_ids=[0],  # Specify partition IDs for synthetic data\n",
    "        synthetic_days=50,  # Pass synthetic_days parameter\n",
    "        num_score_dates=5,  # Pass num_score_dates parameter\n",
    "    )\n",
    "elif KAGGLE_TEST:\n",
    "    inference_server = InferenceServer(\n",
    "        model=model,\n",
    "        preprocessor=preprocessor,\n",
    "        dir=DATA_DIR,\n",
    "        test_parquet=f\"{DATA_DIR}/synthetic_test.parquet\",\n",
    "        lag_parquet=f\"{DATA_DIR}/synthetic_lag.parquet\",\n",
    "        cache_lb_days=15,\n",
    "        feature_cols=FEATURE_COLS,\n",
    "        responder_cols=RESPONDER_COLS,\n",
    "        test=True,  # Set to False for Kaggle test\n",
    "        partition_ids=[0],  # Specify partition IDs for synthetic data\n",
    "        synthetic_days=50,  # Pass synthetic_days parameter\n",
    "        num_score_dates=5,  # Pass num_score_dates parameter\n",
    "    )\n",
    "else:\n",
    "    inference_server = InferenceServer(\n",
    "        model=model,\n",
    "        preprocessor=preprocessor,\n",
    "        dir=DATA_DIR,\n",
    "        test_parquet=f\"{DATA_DIR}/test.parquet\",\n",
    "        lag_parquet=f\"{DATA_DIR}/lags.parquet\",\n",
    "        cache_lb_days=15,\n",
    "        feature_cols=FEATURE_COLS,\n",
    "        responder_cols=RESPONDER_COLS,\n",
    "        test=False,  # Set to False for Kaggle submission\n",
    "    )\n",
    "\n",
    "inference_server.run_inference_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-15T18:19:33.699218Z",
     "iopub.status.busy": "2024-12-15T18:19:33.698393Z",
     "iopub.status.idle": "2024-12-15T18:19:33.707550Z",
     "shell.execute_reply": "2024-12-15T18:19:33.706568Z"
    },
    "papermill": {
     "duration": 0.013835,
     "end_time": "2024-12-15T18:19:33.709360",
     "exception": false,
     "start_time": "2024-12-15T18:19:33.695525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-15T18:19:33.717708Z",
     "iopub.status.busy": "2024-12-15T18:19:33.716970Z",
     "iopub.status.idle": "2024-12-15T18:19:34.004467Z",
     "shell.execute_reply": "2024-12-15T18:19:34.003573Z"
    },
    "papermill": {
     "duration": 0.292566,
     "end_time": "2024-12-15T18:19:34.006977",
     "exception": false,
     "start_time": "2024-12-15T18:19:33.714411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "js_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.52578,
   "end_time": "2024-12-15T18:19:34.528618",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-15T18:19:30.002838",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
