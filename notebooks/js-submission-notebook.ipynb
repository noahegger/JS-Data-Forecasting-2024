{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-15T18:19:32.510681Z",
     "iopub.status.busy": "2024-12-15T18:19:32.510255Z",
     "iopub.status.idle": "2024-12-15T18:19:33.687303Z",
     "shell.execute_reply": "2024-12-15T18:19:33.686245Z"
    },
    "papermill": {
     "duration": 1.18424,
     "end_time": "2024-12-15T18:19:33.689733",
     "exception": false,
     "start_time": "2024-12-15T18:19:32.505493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import kaggle_evaluation.jane_street_inference_server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.001737,
     "end_time": "2024-12-15T18:19:33.693773",
     "exception": false,
     "start_time": "2024-12-15T18:19:33.692036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The evaluation API requires that you set up a server which will respond to inference requests. We have already defined the server; you just need write the predict function. When we evaluate your submission on the hidden test set the client defined in `jane_street_gateway` will run in a different container with direct access to the hidden test set and hand off the data timestep by timestep.\n",
    "\n",
    "\n",
    "\n",
    "Your code will always have access to the published copies of the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Classes/Methods/Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        symbol_id: Optional[int] = None,\n",
    "        responder: int = 6,\n",
    "        partition_ids: Optional[list[int]] = None,\n",
    "        feature_set: Optional[list] = None,\n",
    "        sample_frequency: int = 15,\n",
    "        exclude_set: list = [\n",
    "            \"feature_00\",\n",
    "            \"feature_01\",\n",
    "            \"feature_02\",\n",
    "            \"feature_03\",\n",
    "            \"feature_04\",\n",
    "            \"feature_21\",\n",
    "            \"feature_26\",\n",
    "            \"feature_27\",\n",
    "            \"feature_31\",\n",
    "        ],\n",
    "    ):\n",
    "        self.symbol_id = symbol_id\n",
    "        self.responder = responder\n",
    "        self.partition_ids = partition_ids\n",
    "        self.feature_set = feature_set\n",
    "        self.sample_frequency = sample_frequency\n",
    "        self.exclude_set = exclude_set\n",
    "\n",
    "    def filter_symbol(self, df: pd.DataFrame, symbol_id: int):\n",
    "        return df[df[\"symbol_id\"] == symbol_id]\n",
    "\n",
    "    def resample(self, df: pd.DataFrame, sample_frequency: int):\n",
    "        return (\n",
    "            df.set_index(\"time_index\")\n",
    "            .resample(f\"{sample_frequency}min\")\n",
    "            .first()\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "    def create_time_index(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "        # Convert to numpy.int32 to prevent overflow\n",
    "        df[\"date_id\"] = df[\"date_id\"].astype(\"int32\")\n",
    "        df[\"time_id\"] = df[\"time_id\"].astype(\"int32\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def read_partition(self, read_all=False) -> pd.DataFrame:\n",
    "        if self.partition_ids:\n",
    "            if read_all:\n",
    "                df = pd.concat([pd.read_parquet(parquet) for parquet in train_parquets])\n",
    "            else:\n",
    "                dfs = []\n",
    "                for partition_id in self.partition_ids:\n",
    "                    dfs.append(pd.read_parquet(train_parquets[partition_id]))\n",
    "                df = pd.concat(dfs, ignore_index=True)\n",
    "        else:\n",
    "            df = pd.read_parquet(test_parquet)\n",
    "\n",
    "        df = self.create_time_index(df)\n",
    "        if self.symbol_id:\n",
    "            df = self.filter_symbol(df, self.symbol_id)\n",
    "        df = self.resample(df, self.sample_frequency)\n",
    "\n",
    "        if self.exclude_set:\n",
    "            df.drop(columns=self.exclude_set, inplace=True)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_time_weights(n, halflife=0.35):\n",
    "    decay_factor = 0.5 ** (1 / (halflife * n))\n",
    "    weights = decay_factor ** np.arange(n)\n",
    "    weights /= weights.sum()\n",
    "    return weights\n",
    "\n",
    "def time_weighted_mean(vals, n, halflife=0.35):\n",
    "    weights = get_time_weights(n, halflife)\n",
    "    return np.dot(vals, weights[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ExpWeightedMeanCalculator:\n",
    "    def __init__(self, halflife=0.35, lookback=15, max_nans=5, replace=True):\n",
    "        self.halflife = halflife\n",
    "        self.lookback = lookback\n",
    "        self.max_nans = max_nans\n",
    "        self.replace = replace\n",
    "\n",
    "    def calculate(self, df: pd.DataFrame, tdate: int, feature_column: str) -> float:\n",
    "        lookback_dates = range(tdate - self.lookback, tdate)\n",
    "        mean_values = []\n",
    "\n",
    "        for date in lookback_dates:\n",
    "            daily_mean = df[df[\"date_id\"] == date][feature_column].mean()\n",
    "            mean_values.append(daily_mean)\n",
    "\n",
    "        nan_count = sum(pd.isna(mean_values))\n",
    "        if nan_count > self.max_nans:\n",
    "            return 0\n",
    "\n",
    "        if self.replace:\n",
    "            mean_values = [0 if pd.isna(m) else m for m in mean_values]\n",
    "\n",
    "        res = time_weighted_mean(mean_values, self.lookback, self.halflife)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy-Pasted Definitions (until I figure out how to import scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-15T18:19:33.699218Z",
     "iopub.status.busy": "2024-12-15T18:19:33.698393Z",
     "iopub.status.idle": "2024-12-15T18:19:33.707550Z",
     "shell.execute_reply": "2024-12-15T18:19:33.706568Z"
    },
    "papermill": {
     "duration": 0.013835,
     "end_time": "2024-12-15T18:19:33.709360",
     "exception": false,
     "start_time": "2024-12-15T18:19:33.695525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lags_ : pl.DataFrame | None = None\n",
    "accumulated_data : pl.DataFrame = pl.DataFrame()\n",
    "ewma_calculator = ExpWeightedMeanCalculator(halflife=0.35, lookback=15)\n",
    "\n",
    "# Replace this function with your inference code.\n",
    "# You can return either a Pandas or Polars dataframe, though Polars is recommended.\n",
    "# Each batch of predictions (except the very first) must be returned within 1 minute of the batch features being provided.\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    global lags_, accumulated_data\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "\n",
    "    # Append new test data to accumulated_data\n",
    "    accumulated_data = pl.concat([accumulated_data, test])\n",
    "\n",
    "    # Check if we have accumulated 15 days of data\n",
    "    if accumulated_data['date_id'].n_unique() < 15:\n",
    "        # Use the mean of responder_6_lag_1 from lags\n",
    "        mean_responder_6_lag_1 = lags_['responder_6_lag_1'].mean()\n",
    "        predictions = test.select(\n",
    "            'row_id',\n",
    "            pl.lit(mean_responder_6_lag_1).alias('responder_6'),\n",
    "        )\n",
    "    else:\n",
    "        # Use the ExpWeightedMeanCalculator\n",
    "        resampled_data = Preprocessor(sample_frequency = 15).resample(accumulated_data)\n",
    "        tdate = test['date_id'].max()\n",
    "        ewma_value = ewma_calculator.calculate(accumulated_data.to_pandas(), tdate, 'responder_6')\n",
    "        predictions = test.select(\n",
    "            'row_id',\n",
    "            pl.lit(ewma_value).alias('responder_6'),\n",
    "        )\n",
    "\n",
    "    if isinstance(predictions, pl.DataFrame):\n",
    "        assert predictions.columns == ['row_id', 'responder_6']\n",
    "    elif isinstance(predictions, pd.DataFrame):\n",
    "        assert (predictions.columns == ['row_id', 'responder_6']).all()\n",
    "    else:\n",
    "        raise TypeError('The predict function must return a DataFrame')\n",
    "    # Confirm has as many rows as the test data.\n",
    "    assert len(predictions) == len(test)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.001528,
     "end_time": "2024-12-15T18:19:33.712761",
     "exception": false,
     "start_time": "2024-12-15T18:19:33.711233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "When your notebook is run on the hidden test set, inference_server.serve must be called within 15 minutes of the notebook starting or the gateway will throw an error. If you need more than 15 minutes to load your model you can do so during the very first `predict` call, which does not have the usual 1 minute response deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-15T18:19:33.717708Z",
     "iopub.status.busy": "2024-12-15T18:19:33.716970Z",
     "iopub.status.idle": "2024-12-15T18:19:34.004467Z",
     "shell.execute_reply": "2024-12-15T18:19:34.003573Z"
    },
    "papermill": {
     "duration": 0.292566,
     "end_time": "2024-12-15T18:19:34.006977",
     "exception": false,
     "start_time": "2024-12-15T18:19:33.714411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "js_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.52578,
   "end_time": "2024-12-15T18:19:34.528618",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-15T18:19:30.002838",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
