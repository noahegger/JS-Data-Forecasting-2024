{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806344a7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-08T20:13:26.010324Z",
     "iopub.status.busy": "2025-01-08T20:13:26.009930Z",
     "iopub.status.idle": "2025-01-08T20:13:28.497093Z",
     "shell.execute_reply": "2025-01-08T20:13:28.495544Z"
    },
    "papermill": {
     "duration": 2.494741,
     "end_time": "2025-01-08T20:13:28.499405",
     "exception": false,
     "start_time": "2025-01-08T20:13:26.004664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import defaultdict, deque\n",
    "from pathlib import Path\n",
    "from typing import IO, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define global variables for the project path\n",
    "DATA_DIR = Path(\"/kaggle/input/jane-street-real-time-market-data-forecasting\")\n",
    "LOCAL_DATA_DIR = None  # Not used on Kaggle\n",
    "PROJECT_PATH = Path(\"/kaggle/input/js-scripts\")  # Adjusted path for Kaggle\n",
    "sys.path.append(str(PROJECT_PATH))  # Add the dataset root to sys.path\n",
    "\n",
    "META_COLS = [\"date_id\", \"time_id\", \"symbol_id\"] #, \"weight\"]\n",
    "FEATURE_COLS = [f\"feature_{x:02}\" for x in range(79)]\n",
    "RESPONDER_COLS = [f\"responder_{i}\" for i in range(9)]\n",
    "\n",
    "# Import directly from the dataset root\n",
    "from calculators import (\n",
    "    ExpWeightedMeanCalculator,\n",
    "    MovingAverageCalculator,\n",
    "    OnlineMovingAverageCalculator,\n",
    "    RevDecayCalculator,\n",
    ")\n",
    "from models import BaseModel, EnsembleTimeSeriesV1\n",
    "from data_preprocessing import Preprocessor\n",
    "from record import Cache, SymbolRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e3b154",
   "metadata": {
    "papermill": {
     "duration": 0.002398,
     "end_time": "2025-01-08T20:13:28.505456",
     "exception": false,
     "start_time": "2025-01-08T20:13:28.503058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The evaluation API requires that you set up a server which will respond to inference requests. We have already defined the server; you just need write the predict function. When we evaluate your submission on the hidden test set the client defined in `jane_street_gateway` will run in a different container with direct access to the hidden test set and hand off the data timestep by timestep.\n",
    "\n",
    "\n",
    "\n",
    "Your code will always have access to the published copies of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba62793a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T20:13:28.512665Z",
     "iopub.status.busy": "2025-01-08T20:13:28.512008Z",
     "iopub.status.idle": "2025-01-08T20:13:28.548328Z",
     "shell.execute_reply": "2025-01-08T20:13:28.546663Z"
    },
    "papermill": {
     "duration": 0.043315,
     "end_time": "2025-01-08T20:13:28.551280",
     "exception": false,
     "start_time": "2025-01-08T20:13:28.507965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        preprocessor,\n",
    "        dir: Optional[Path] = None,\n",
    "        test_parquet: Optional[str] = None,\n",
    "        lag_parquet: Optional[str] = None,\n",
    "        cache_lb_days: int = 15,\n",
    "        feature_cols: List[str] = FEATURE_COLS,\n",
    "        responder_cols: List[str] = RESPONDER_COLS,\n",
    "        test: bool = False,\n",
    "        partition_ids: Optional[List[int]] = None,\n",
    "        synthetic_days: int = 50,\n",
    "        num_score_dates: int = 5,\n",
    "        date_offset: int = 1690,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.dir = dir\n",
    "        self.test_parquet = str(test_parquet) if test_parquet else \"\"\n",
    "        self.lag_parquet = str(lag_parquet) if lag_parquet else \"\"\n",
    "        self.cache_lb_days = cache_lb_days\n",
    "        self.feature_cols = feature_cols\n",
    "        self.responder_cols = responder_cols\n",
    "        self.cache_history = Cache(maxlen=self.cache_lb_days)\n",
    "        self.lag_cache = Cache(maxlen=self.cache_lb_days)\n",
    "        self.test_ = None\n",
    "        self.time_step_count = 0\n",
    "        self.test = test\n",
    "        self.partition_ids = partition_ids\n",
    "        self.synthetic_days = synthetic_days\n",
    "        self.num_score_dates = num_score_dates\n",
    "        self.date_offset = date_offset\n",
    "\n",
    "        self.ensure_parquet_files()\n",
    "        self.total_time_steps = self.calculate_total_time_steps()\n",
    "        self.initialize_pbar()\n",
    "\n",
    "        if not self.test:\n",
    "            last_train_set = pl.read_parquet(\n",
    "                f\"{self.dir}/train.parquet/partition_id=9/part-0.parquet\"\n",
    "            )\n",
    "            self.initialize_cache(\n",
    "                last_train_set.filter(\n",
    "                    pl.col(\"date_id\") >= (self.date_offset - self.cache_lb_days)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def ensure_parquet_files(self):\n",
    "        if self.test and (\n",
    "            not self.test_parquet\n",
    "            or not Path(self.test_parquet).exists()\n",
    "            or not self.lag_parquet\n",
    "            or not Path(self.lag_parquet).exists()\n",
    "        ):\n",
    "            self.test_parquet, self.lag_parquet = self.generate_synthetic_data()\n",
    "\n",
    "    def generate_synthetic_data(self):\n",
    "        train_parquets = [\n",
    "            f\"{self.dir}/train.parquet/partition_id={i}/part-0.parquet\"\n",
    "            for i in (self.partition_ids or range(10))\n",
    "        ]\n",
    "        train_data = pl.concat([pl.read_parquet(parquet) for parquet in train_parquets])\n",
    "\n",
    "        if self.synthetic_days:\n",
    "            date_ids = sorted(train_data[\"date_id\"].unique())[: self.synthetic_days]\n",
    "            test_data, lag_data = (\n",
    "                train_data.filter(pl.col(\"date_id\").is_in(date_ids)),\n",
    "                train_data.filter(pl.col(\"date_id\").is_in(date_ids)),\n",
    "            )\n",
    "        else:\n",
    "            test_data, lag_data = train_data, train_data\n",
    "\n",
    "        # Adjust date_id for test_data and lag_data\n",
    "        test_data = test_data.with_columns(\n",
    "            (pl.col(\"date_id\") - self.cache_lb_days).alias(\"date_id\")\n",
    "        ).with_row_index(name=\"row_id\", offset=0)\n",
    "\n",
    "        lag_data = lag_data.with_columns(\n",
    "            (pl.col(\"date_id\") - self.cache_lb_days).alias(\"date_id\")\n",
    "        )\n",
    "\n",
    "        # Add \"is_scored\" column to test_data\n",
    "        test_data = test_data.with_columns(\n",
    "            pl.when(pl.col(\"date_id\") < 0)\n",
    "            .then(False)\n",
    "            .otherwise(True)\n",
    "            .alias(\"is_scored\")\n",
    "        )\n",
    "\n",
    "        # Initialize the cache with test data where date_id < 0\n",
    "        self.initialize_cache(test_data.filter(pl.col(\"date_id\") < 0))\n",
    "\n",
    "        # Select relevant columns for test_data and lag_data\n",
    "        test_data = test_data.select(\n",
    "            [\"row_id\"] + META_COLS + [\"is_scored\"] + FEATURE_COLS\n",
    "        )\n",
    "        lag_data = lag_data.select(META_COLS + RESPONDER_COLS)\n",
    "\n",
    "        # Define paths for synthetic data\n",
    "        test_parquet_path = Path(self.test_parquet)\n",
    "        lag_parquet_path = Path(self.lag_parquet)\n",
    "\n",
    "        # Create directories if they don't exist\n",
    "        test_parquet_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        lag_parquet_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Partition and save test_data\n",
    "        test_data_partition = test_data.partition_by(\n",
    "            \"date_id\", maintain_order=True, as_dict=True\n",
    "        )\n",
    "        row_id_offset = (\n",
    "            test_data.filter(pl.col(\"date_id\") < 0).select(\"row_id\").max().item()\n",
    "        )\n",
    "\n",
    "        for key, _df in test_data_partition.items():\n",
    "            date_id = key[0]\n",
    "            if date_id >= 0:  # type: ignore\n",
    "                partition_dir = test_parquet_path / f\"date_id={date_id}\"\n",
    "                partition_dir.mkdir(parents=True, exist_ok=True)\n",
    "                _df = _df.with_columns(pl.col(\"row_id\") - row_id_offset)\n",
    "                _df.write_parquet(partition_dir / \"part-0.parquet\")\n",
    "\n",
    "        # Partition and save lag_data\n",
    "        lag_data = lag_data.rename(\n",
    "            {f\"responder_{x}\": f\"responder_{x}_lag_1\" for x in range(9)}\n",
    "        )\n",
    "        lag_data_partition = lag_data.partition_by(\n",
    "            \"date_id\", maintain_order=True, as_dict=True\n",
    "        )\n",
    "\n",
    "        for key, _df in lag_data_partition.items():\n",
    "            date_id = key[0] + 1  # type: ignore\n",
    "            if date_id >= 0:  # type: ignore\n",
    "                partition_dir = lag_parquet_path / f\"date_id={date_id}\"\n",
    "                partition_dir.mkdir(parents=True, exist_ok=True)\n",
    "                _df = _df.with_columns((pl.col(\"date_id\") + 1).alias(\"date_id\"))\n",
    "                _df.write_parquet(partition_dir / \"part-0.parquet\")\n",
    "\n",
    "        return str(test_parquet_path), str(lag_parquet_path)\n",
    "\n",
    "    def calculate_total_time_steps(self):\n",
    "        if self.test and self.test_parquet:\n",
    "            return (\n",
    "                pl.scan_parquet(self.test_parquet)\n",
    "                .select((pl.col(\"date_id\") * 10000 + pl.col(\"time_id\")).n_unique())\n",
    "                .collect()\n",
    "                .item()\n",
    "            )\n",
    "        return 1  # For live submission\n",
    "\n",
    "    def initialize_pbar(self):\n",
    "        self.pbar = tqdm(total=self.total_time_steps)\n",
    "\n",
    "    def generate_data_batches(self, test_data: pl.DataFrame, lag_data: pl.DataFrame):\n",
    "        date_ids = sorted(test_data[\"date_id\"].unique())\n",
    "        assert date_ids[0] == 0\n",
    "\n",
    "        for date_id in date_ids:\n",
    "            test_batches = test_data.filter(pl.col(\"date_id\") == date_id).group_by(\n",
    "                \"time_id\", maintain_order=True\n",
    "            )\n",
    "            lags = lag_data.filter(pl.col(\"date_id\") == date_id)\n",
    "\n",
    "            for (time_id,), test in test_batches:\n",
    "                test_batch = (test, lags if time_id == 0 else None)\n",
    "                validation_data = test.select(\"row_id\")  # row_id in gateway?\n",
    "                yield test_batch, validation_data\n",
    "\n",
    "    def run_inference_server(self):\n",
    "        self.pbar.refresh()\n",
    "\n",
    "        if self.test:\n",
    "            if self.lag_parquet and self.test_parquet:\n",
    "                lag_data = pl.scan_parquet(f\"{self.lag_parquet}/**/*.parquet\").collect()\n",
    "                test_data = pl.scan_parquet(\n",
    "                    f\"{self.test_parquet}/**/*.parquet\"\n",
    "                ).collect()\n",
    "            else:\n",
    "                raise ValueError(\"lag_parquet or test_parquet is None\")\n",
    "            for test_batch, validation_data in self.generate_data_batches(\n",
    "                test_data, lag_data\n",
    "            ):\n",
    "                test, lags = test_batch\n",
    "                if lags is None:\n",
    "                    lags = pl.DataFrame()\n",
    "                self.predict(test, lags)\n",
    "        else:\n",
    "            import kaggle_evaluation.jane_street_inference_server as js_server\n",
    "            inference_server = js_server.JSInferenceServer(self.predict)\n",
    "\n",
    "            if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "                inference_server.serve()\n",
    "            else:\n",
    "                print(\"worked\")\n",
    "                inference_server.run_local_gateway(\n",
    "                    (self.test_parquet, self.lag_parquet)\n",
    "                )\n",
    "\n",
    "        self.pbar.close()\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        test: pl.DataFrame,\n",
    "        lags: Optional[pl.DataFrame] = None,\n",
    "    ) -> pl.DataFrame | pd.DataFrame:\n",
    "\n",
    "        if lags is not None:\n",
    "            lag_responder_cols = [f\"{col}_lag_1\" for col in self.responder_cols]\n",
    "            for (symbol_id,), batch in lags.group_by(\"symbol_id\", maintain_order=True):\n",
    "\n",
    "                batch_data = batch.select(META_COLS + lag_responder_cols)\n",
    "                date_id = batch[\"date_id\"][0]\n",
    "                self.lag_cache.update(symbol_id, date_id, batch_data, is_lag_cache=True)  # type: ignore\n",
    "\n",
    "        self.test_ = test\n",
    "\n",
    "        if test[\"is_scored\"].any():\n",
    "            try:\n",
    "                symbol_ids = []\n",
    "                row_ids = []\n",
    "                for (symbol_id,), batch in test.group_by(\n",
    "                    \"symbol_id\", maintain_order=True\n",
    "                ):\n",
    "                    batch_data = batch.select(META_COLS + self.feature_cols)\n",
    "                    date_id = batch[\"date_id\"][0]\n",
    "                    self.cache_history.update(symbol_id, date_id, batch_data)  # type: ignore\n",
    "\n",
    "                    symbol_ids.append(symbol_id)\n",
    "                    row_ids.append(batch[\"row_id\"][-1])\n",
    "\n",
    "                # Extract the most recent date_id\n",
    "                tdate = test[\"date_id\"][-1]\n",
    "                ttime = batch[\"time_id\"][-1]\n",
    "                # Pass the cache history to the prediction model\n",
    "                estimates = self.model.get_estimates(\n",
    "                    symbol_ids=symbol_ids,\n",
    "                    cache_history=self.cache_history.cache,\n",
    "                    lag_cache=self.lag_cache.cache,\n",
    "                    tdate=tdate,\n",
    "                    ttime=ttime,\n",
    "                )\n",
    "\n",
    "                predictions = pl.DataFrame(\n",
    "                    {\n",
    "                        \"row_id\": row_ids,\n",
    "                        \"responder_6\": estimates,\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "        else:\n",
    "            predictions = pl.DataFrame(\n",
    "                {\"row_id\": test[\"row_id\"], \"responder_6\": [0] * len(test)}\n",
    "            )\n",
    "        \n",
    "        assert isinstance(predictions, pl.DataFrame | pd.DataFrame)\n",
    "        assert list(predictions.columns) == [\"row_id\", \"responder_6\"]\n",
    "        assert len(predictions) == len(test)\n",
    "\n",
    "        self.time_step_count += 1\n",
    "        self.pbar.update(1)\n",
    "        print(predictions)\n",
    "        return predictions\n",
    "\n",
    "    def initialize_cache(self, data: pl.DataFrame):\n",
    "        self.cache_history.initialize(data, META_COLS + self.feature_cols)\n",
    "        self.lag_cache.initialize(data, META_COLS + self.responder_cols, lagged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a53bad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T20:13:28.559339Z",
     "iopub.status.busy": "2025-01-08T20:13:28.558908Z",
     "iopub.status.idle": "2025-01-08T20:13:39.362883Z",
     "shell.execute_reply": "2025-01-08T20:13:39.361367Z"
    },
    "papermill": {
     "duration": 10.810195,
     "end_time": "2025-01-08T20:13:39.364867",
     "exception": false,
     "start_time": "2025-01-08T20:13:28.554672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:10<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (39, 2)\n",
      "┌────────┬─────────────┐\n",
      "│ row_id ┆ responder_6 │\n",
      "│ ---    ┆ ---         │\n",
      "│ i64    ┆ f64         │\n",
      "╞════════╪═════════════╡\n",
      "│ 0      ┆ -0.236546   │\n",
      "│ 1      ┆ -0.198116   │\n",
      "│ 2      ┆ -0.208089   │\n",
      "│ 3      ┆ 0.056808    │\n",
      "│ 4      ┆ 0.037784    │\n",
      "│ …      ┆ …           │\n",
      "│ 34     ┆ -0.074476   │\n",
      "│ 35     ┆ 0.096605    │\n",
      "│ 36     ┆ -0.439002   │\n",
      "│ 37     ┆ -0.046576   │\n",
      "│ 38     ┆ -0.071187   │\n",
      "└────────┴─────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LOCAL_TEST = False\n",
    "KAGGLE_TEST = False\n",
    "\n",
    "model = EnsembleTimeSeriesV1(\n",
    "    online_feature=OnlineMovingAverageCalculator(window=10),\n",
    "    long_term_feature=ExpWeightedMeanCalculator(halflife=0.35, lookback=15),\n",
    "    rev_decay_calculator=RevDecayCalculator(lookback=15),\n",
    "    st_window=15,\n",
    "    lt_window=15,\n",
    ")\n",
    "preprocessor = Preprocessor(\n",
    "    symbol_id=None,\n",
    "    responder=6,\n",
    "    partition_ids=None,\n",
    "    feature_set=None,\n",
    "    sample_frequency=15,\n",
    "    exclude_set=[\n",
    "        \"feature_00\",\n",
    "        \"feature_01\",\n",
    "        \"feature_02\",\n",
    "        \"feature_03\",\n",
    "        \"feature_04\",\n",
    "        \"feature_21\",\n",
    "        \"feature_26\",\n",
    "        \"feature_27\",\n",
    "        \"feature_31\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "predictor = Predictor(\n",
    "    model=model,\n",
    "    preprocessor=preprocessor,\n",
    "    dir=DATA_DIR,\n",
    "    test_parquet=f\"{DATA_DIR}/test.parquet\",\n",
    "    lag_parquet=f\"{DATA_DIR}/lags.parquet\",\n",
    "    cache_lb_days=15,\n",
    "    feature_cols=FEATURE_COLS,\n",
    "    responder_cols=RESPONDER_COLS,\n",
    "    test=False,  # Set to False for Kaggle submission\n",
    ")\n",
    "\n",
    "predictor.run_inference_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422fbf4f",
   "metadata": {
    "papermill": {
     "duration": 0.003076,
     "end_time": "2025-01-08T20:13:39.371856",
     "exception": false,
     "start_time": "2025-01-08T20:13:39.368780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a6268",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-15T18:19:33.699218Z",
     "iopub.status.busy": "2024-12-15T18:19:33.698393Z",
     "iopub.status.idle": "2024-12-15T18:19:33.707550Z",
     "shell.execute_reply": "2024-12-15T18:19:33.706568Z"
    },
    "papermill": {
     "duration": 0.002872,
     "end_time": "2025-01-08T20:13:39.378053",
     "exception": false,
     "start_time": "2025-01-08T20:13:39.375181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832d2f8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-15T18:19:33.717708Z",
     "iopub.status.busy": "2024-12-15T18:19:33.716970Z",
     "iopub.status.idle": "2024-12-15T18:19:34.004467Z",
     "shell.execute_reply": "2024-12-15T18:19:34.003573Z"
    },
    "papermill": {
     "duration": 0.002808,
     "end_time": "2025-01-08T20:13:39.383997",
     "exception": false,
     "start_time": "2025-01-08T20:13:39.381189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    },
    {
     "datasetId": 6443049,
     "sourceId": 10398515,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.01246,
   "end_time": "2025-01-08T20:13:40.612171",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-08T20:13:23.599711",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
